{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de Mascarillas: CNN Personalizada y Transfer Learning\n",
    "\n",
    "**Análisis y Comparación de Arquitecturas para la Localización de Objetos**\n",
    "\n",
    "Este notebook aborda el problema de la detección de mascarillas a través de dos enfoques principales:\n",
    "1.  **CNN Personalizada:** Se diseña, entrena y evalúa una Red Neuronal Convolucional desde cero.\n",
    "2.  **Transfer Learning:** Se adaptan dos modelos pre-entrenados (EfficientNet-B0 y Swin Transformer V2) para la misma tarea.\n",
    "\n",
    "Se explorará el impacto del **aumento de datos** y el ajuste de **hiperparámetros** como el learning rate y el optimizador para mejorar el rendimiento en la clasificación y la regresión del cuadro delimitador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando el dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# CONFIGURACIÓN INICIAL\n",
    "# ===========================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import os\n",
    "import os.path as osp\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import typing as ty\n",
    "import copy\n",
    "from functools import reduce\n",
    "\n",
    "# Ignorar advertencias para una salida más limpia\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de reproducibilidad y dispositivo\n",
    "torch.manual_seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Usando el dispositivo: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructura del DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>class</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>video_CDC-YOUTUBE_mp4-63_jpg.rf.2f4f64f6ef712f...</td>\n",
       "      <td>315</td>\n",
       "      <td>249</td>\n",
       "      <td>468</td>\n",
       "      <td>374</td>\n",
       "      <td>no-mask</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG_4860_mp4-36_jpg.rf.01a053cabddff2cdd19f04e...</td>\n",
       "      <td>257</td>\n",
       "      <td>237</td>\n",
       "      <td>299</td>\n",
       "      <td>264</td>\n",
       "      <td>no-mask</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG_1491_mp4-12_jpg.rf.9df64033aebef44b8bb9a6a...</td>\n",
       "      <td>291</td>\n",
       "      <td>245</td>\n",
       "      <td>582</td>\n",
       "      <td>449</td>\n",
       "      <td>mask</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMG_4861_mp4-64_jpg.rf.74ab6d1da8a1fa9b8fbb576...</td>\n",
       "      <td>231</td>\n",
       "      <td>229</td>\n",
       "      <td>577</td>\n",
       "      <td>420</td>\n",
       "      <td>no-mask</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMG_9950-1-_mp4-83_jpg.rf.74dca33810c23ba144d8...</td>\n",
       "      <td>107</td>\n",
       "      <td>168</td>\n",
       "      <td>515</td>\n",
       "      <td>469</td>\n",
       "      <td>no-mask</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  xmin  ymin  xmax  ymax  \\\n",
       "0  video_CDC-YOUTUBE_mp4-63_jpg.rf.2f4f64f6ef712f...   315   249   468   374   \n",
       "1  IMG_4860_mp4-36_jpg.rf.01a053cabddff2cdd19f04e...   257   237   299   264   \n",
       "2  IMG_1491_mp4-12_jpg.rf.9df64033aebef44b8bb9a6a...   291   245   582   449   \n",
       "3  IMG_4861_mp4-64_jpg.rf.74ab6d1da8a1fa9b8fbb576...   231   229   577   420   \n",
       "4  IMG_9950-1-_mp4-83_jpg.rf.74dca33810c23ba144d8...   107   168   515   469   \n",
       "\n",
       "     class  class_id  \n",
       "0  no-mask         0  \n",
       "1  no-mask         0  \n",
       "2     mask         1  \n",
       "3  no-mask         0  \n",
       "4  no-mask         0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===========================\n",
    "# CARGA Y PREPARACIÓN DE DATOS\n",
    "# ===========================\n",
    "\n",
    "# Rutas de los directorios\n",
    "DATA_DIR = './kaggle/input/aa-iv-2025-ii-object-localization'\n",
    "IMG_DIR = osp.join(DATA_DIR, \"images\")\n",
    "\n",
    "# Carga del archivo de anotaciones\n",
    "df = pd.read_csv(osp.join(DATA_DIR, \"train.csv\"))\n",
    "\n",
    "# Mapeo de clases a IDs numéricos y viceversa\n",
    "obj2id = {'no-mask': 0, 'mask': 1}\n",
    "id2obj = {0: 'no-mask', 1: 'mask'}\n",
    "\n",
    "df[\"class_id\"] = df[\"class\"].map(obj2id)\n",
    "\n",
    "# Selección de columnas relevantes\n",
    "df = df[['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'class', 'class_id']].copy()\n",
    "\n",
    "print(\"Estructura del DataFrame:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 175 muestras\n",
      "Tamaño del conjunto de validación: 44 muestras\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# PREPROCESAMIENTO\n",
    "# ===========================\n",
    "\n",
    "# Normalización de las coordenadas del Bounding Box\n",
    "h_real, w_real = 640, 640  # Dimensiones de las imágenes\n",
    "df[[\"ymin\", \"ymax\"]] = df[[\"ymin\", \"ymax\"]].div(h_real, axis=0)\n",
    "df[[\"xmin\", \"xmax\"]] = df[[\"xmin\", \"xmax\"]].div(w_real, axis=0)\n",
    "\n",
    "# División estratificada en conjuntos de entrenamiento y validación\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    stratify=df['class_id'],\n",
    "    test_size=0.20, # 20% para validación\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {train_df.shape[0]} muestras\")\n",
    "print(f\"Tamaño del conjunto de validación: {val_df.shape[0]} muestras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# CLASE DATASET DE PYTORCH (CORREGIDA V2)\n",
    "# ===========================\n",
    "\n",
    "class MaskDataset(Dataset):\n",
    "    \"\"\"Dataset para cargar las imágenes, bounding boxes y clases.\"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, root_dir: str, transform=None, labeled: bool = True):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.labeled = labeled\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Cargar imagen\n",
    "        img_name = os.path.join(self.root_dir, self.df.filename.iloc[idx])\n",
    "        image = cv2.imread(img_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        sample = {'image': image}\n",
    "\n",
    "        if self.labeled:\n",
    "            bbox = self.df.iloc[idx, 1:5].values.astype('float')\n",
    "            class_id = self.df.class_id.iloc[idx]\n",
    "            sample.update({'bbox': np.array([bbox]), 'class_id': np.array([class_id])})\n",
    "\n",
    "        if self.transform:\n",
    "            transform_args = {'image': sample['image']}\n",
    "            if self.labeled:\n",
    "                transform_args['bboxes'] = sample['bbox']\n",
    "                transform_args['category_ids'] = [sample['class_id'].item()]\n",
    "            \n",
    "            transformed = self.transform(**transform_args)\n",
    "            \n",
    "            sample['image'] = transformed['image']\n",
    "            if self.labeled:\n",
    "                # === INICIO DE LA CORRECCIÓN ===\n",
    "                # Comprobamos la longitud del contenedor para ver si hay bboxes,\n",
    "                # en lugar de evaluar la \"veracidad\" del array.\n",
    "                if len(transformed['bboxes']) > 0:\n",
    "                    # Albumentations devuelve una tupla por bbox, tomamos solo las coordenadas.\n",
    "                    bbox_coords = transformed['bboxes'][0][:4]\n",
    "                    sample['bbox'] = torch.tensor(bbox_coords, dtype=torch.float32)\n",
    "                else:\n",
    "                    # Si el aumento de datos eliminó el bbox, creamos un tensor vacío.\n",
    "                    sample['bbox'] = torch.zeros(4, dtype=torch.float32)\n",
    "                # === FIN DE LA CORRECCIÓN ===\n",
    "        \n",
    "        # Convertir imagen a tensor y permutar dimensiones (H,W,C) -> (C,H,W)\n",
    "        sample['image'] = torch.from_numpy(sample['image'].transpose((2, 0, 1))).float()\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aumento de Datos (Data Augmentation)\n",
    "\n",
    "Se definen tres niveles de aumento de datos para comparar su impacto:\n",
    "\n",
    "1.  **Aumento Ligero (`light_augmentations`):**\n",
    "    *   `HorizontalFlip`: La técnica más común y segura. Refleja la imagen horizontalmente, lo cual es una variación muy probable en escenarios reales (personas vistas de izquierda o derecha). Ayuda al modelo a ser invariante a la orientación.\n",
    "\n",
    "2.  **Aumento Medio (`medium_augmentations`):**\n",
    "    *   Incluye el `HorizontalFlip`.\n",
    "    *   `RandomBrightnessContrast`: Simula diferentes condiciones de iluminación. Es útil porque las fotos pueden ser tomadas en interiores, exteriores, con o sin flash. Entrena al modelo para que no dependa del brillo o contraste específico de la imagen.\n",
    "    *   `Rotate`: Introduce pequeñas rotaciones. Las cabezas de las personas no siempre están perfectamente verticales. Esto ayuda al modelo a reconocer objetos aunque estén ligeramente inclinados.\n",
    "\n",
    "3.  **Aumento Pesado (`heavy_augmentations`):**\n",
    "    *   Incluye las transformaciones anteriores.\n",
    "    *   `Blur`: Simula imágenes ligeramente desenfocadas o con movimiento, lo que puede ocurrir con cámaras de baja calidad o movimiento rápido.\n",
    "    *   `CoarseDropout`: Elimina regiones rectangulares de la imagen, forzando al modelo a aprender de características parciales del objeto y a no depender de una sola región específica (como los ojos o la nariz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# PIPELINES DE AUMENTO DE DATOS\n",
    "# ===========================\n",
    "IMG_SIZE = 256 # Tamaño de entrada para los modelos\n",
    "\n",
    "# Parámetros comunes para los bounding boxes en Albumentations\n",
    "BBOX_PARAMS = A.BboxParams(format='albumentations', label_fields=['category_ids'])\n",
    "\n",
    "# 1. Aumento Ligero\n",
    "light_augmentations = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "], bbox_params=BBOX_PARAMS)\n",
    "\n",
    "# 2. Aumento Medio\n",
    "medium_augmentations = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Rotate(limit=15, p=0.4),\n",
    "], bbox_params=BBOX_PARAMS)\n",
    "\n",
    "# 3. Aumento Pesado\n",
    "heavy_augmentations = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.4),\n",
    "    A.Rotate(limit=20, p=0.5),\n",
    "    A.Blur(blur_limit=3, p=0.2),\n",
    "    A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.3),\n",
    "], bbox_params=BBOX_PARAMS)\n",
    "\n",
    "# Transformaciones solo para validación (sin aumento)\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "], bbox_params=BBOX_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de Modelos\n",
    "\n",
    "### 1. Backbone: CNN Personalizada\n",
    "Se crea una CNN simple desde cero que servirá como extractor de características. Consta de tres bloques convolucionales, cada uno con una capa `Conv2d`, `ReLU` para la activación y `MaxPool2d` para reducir las dimensiones espaciales. Finalmente, un `AdaptiveAvgPool2d` asegura que la salida tenga un tamaño fijo antes de pasar a las cabezas.\n",
    "\n",
    "### 2. Cabezas de Clasificación y Regresión\n",
    "La clase `Model` es un módulo genérico que integra un `backbone` con dos cabezas:\n",
    "- **Cabeza de Clasificación (`cls_head`):** Una red neuronal simple (MLP) que toma las características del backbone y predice la probabilidad de cada clase (`mask` o `no-mask`).\n",
    "- **Cabeza de Regresión (`reg_head`):** Otro MLP que predice las 4 coordenadas del bounding box (`xmin`, `ymin`, `xmax`, `ymax`).\n",
    "\n",
    "Esta estructura modular permite intercambiar fácilmente el backbone (CNN personalizada, EfficientNet, Swin Transformer).\n",
    "\n",
    "### 3. Backbones para Transfer Learning\n",
    "- **EfficientNet-B0:** Un modelo ligero y eficiente, conocido por su buen equilibrio entre precisión y costo computacional.\n",
    "- **Swin Transformer V2:** Un modelo basado en la arquitectura Transformer, que ha demostrado un rendimiento excelente en tareas de visión por computadora al capturar dependencias a larga distancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# 1. BACKBONE: CNN PERSONALIZADA\n",
    "# ===============================================\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, output_features_dim=256):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2) # 256 -> 128\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2) # 128 -> 64\n",
    "        )\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2) # 64 -> 32\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.final_layer = nn.Linear(128, output_features_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x\n",
    "\n",
    "# ===============================================\n",
    "# 2. CABEZAS DE CLASIFICACIÓN Y REGRESIÓN\n",
    "# ===============================================\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, backbone_out_features: int, n_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # Cabeza de Regresión\n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Linear(backbone_out_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 4) # 4 coordenadas del bbox\n",
    "        )\n",
    "\n",
    "        # Cabeza de Clasificación\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(backbone_out_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, n_classes) # 2 clases\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> ty.Dict[str, Tensor]:\n",
    "        features = self.backbone(x)\n",
    "        pred_bbox = self.reg_head(features)\n",
    "        cls_logits = self.cls_head(features)\n",
    "        return {'bbox': pred_bbox, 'class_id': cls_logits}\n",
    "\n",
    "# ===============================================\n",
    "# 3. BACKBONES PRE-ENTRENADOS\n",
    "# ===============================================\n",
    "class EffNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        m = torchvision.models.efficientnet_b0(weights='DEFAULT')\n",
    "        self.features = m.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n",
    "\n",
    "class SwinV2FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        m = torchvision.models.swin_v2_b(weights='DEFAULT')\n",
    "        self.features = m.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# FUNCIONES AUXILIARES\n",
    "# ===========================\n",
    "\n",
    "def iou(y_true: Tensor, y_pred: Tensor):\n",
    "    \"\"\"Calcula la métrica IoU (Intersection over Union).\"\"\"\n",
    "    # torchvision.ops.box_iou espera [N, 4] y [M, 4]\n",
    "    if y_true.dim() == 3: y_true = y_true.squeeze(1)\n",
    "    if y_pred.dim() == 3: y_pred = y_pred.squeeze(1)\n",
    "    pairwise_iou = torchvision.ops.box_iou(y_true, y_pred)\n",
    "    # Tomamos la diagonal, asumiendo una correspondencia 1 a 1\n",
    "    result = torch.diag(pairwise_iou).mean()\n",
    "    return result\n",
    "\n",
    "def accuracy(y_true: Tensor, y_pred: Tensor):\n",
    "    \"\"\"Calcula la métrica de Accuracy para clasificación.\"\"\"\n",
    "    y_true = y_true.squeeze().long()\n",
    "    pred = torch.argmax(y_pred, dim=-1)\n",
    "    return (pred == y_true).float().mean()\n",
    "\n",
    "def loss_fn(y_true, y_preds, alpha=0.5):\n",
    "    \"\"\"Función de pérdida combinada para clasificación y regresión.\"\"\"\n",
    "    # Pérdida de clasificación (Cross Entropy)\n",
    "    cls_y_true = y_true['class_id'].squeeze().long()\n",
    "    cls_loss = F.cross_entropy(y_preds['class_id'], cls_y_true)\n",
    "\n",
    "    # Pérdida de regresión (Smooth L1 Loss es más robusta a outliers que MSE)\n",
    "    reg_loss = F.smooth_l1_loss(y_preds['bbox'], y_true['bbox'])\n",
    "\n",
    "    total_loss = (1 - alpha) * cls_loss + alpha * reg_loss\n",
    "    return {'loss': total_loss, 'reg_loss': reg_loss, 'cls_loss': cls_loss}\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    \"\"\"Bucle de entrenamiento para una época.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Entrenando\")\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        # Mover datos al dispositivo\n",
    "        for key in ['image', 'bbox', 'class_id']:\n",
    "            batch[key] = batch[key].to(device)\n",
    "\n",
    "        preds = model(batch['image'])\n",
    "        losses = loss_fn(batch, preds)\n",
    "        losses['loss'].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses['loss'].item()\n",
    "        pbar.set_postfix(loss=total_loss/len(pbar))\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Bucle de evaluación.\"\"\"\n",
    "    model.eval()\n",
    "    total_cls_loss, total_reg_loss = 0, 0\n",
    "    all_acc, all_iou = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluando\"):\n",
    "            for key in ['image', 'bbox', 'class_id']:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            preds = model(batch['image'])\n",
    "            losses = loss_fn(batch, preds)\n",
    "            total_cls_loss += losses['cls_loss'].item()\n",
    "            total_reg_loss += losses['reg_loss'].item()\n",
    "\n",
    "            all_acc.append(accuracy(batch['class_id'], preds['class_id']).cpu())\n",
    "            all_iou.append(iou(batch['bbox'], preds['bbox']).cpu())\n",
    "\n",
    "    avg_acc = np.mean(all_acc)\n",
    "    avg_iou = np.mean(all_iou)\n",
    "    avg_combined = (avg_acc + avg_iou) / 2\n",
    "    print(f\"Resultados de validación -> Accuracy: {avg_acc:.4f}, IoU: {avg_iou:.4f}, Average: {avg_combined:.4f}\")\n",
    "    return avg_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 1: CNN Personalizada con Diferentes Aumentos de Datos\n",
    "\n",
    "En este experimento, entrenaremos el modelo con el backbone de CNN personalizada desde cero. El objetivo es analizar cómo los diferentes niveles de aumento de datos (`light`, `medium`, `heavy`) afectan el rendimiento del modelo en ambas tareas (clasificación y regresión)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Entrenando con aumento de datos: LIGHT ---\n",
      "Época 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 6/6 [00:05<00:00,  1.14it/s, loss=0.987]\n",
      "Evaluando: 100%|██████████| 2/2 [00:00<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de validación -> Accuracy: 0.5781, IoU: 0.0994\n",
      "Nuevo mejor modelo de CNN personalizada con Accuracy: 0.0994\n",
      "Época 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:   0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mÉpoca \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_custom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_custom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_custom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     current_acc = evaluate(model_custom, val_loader_custom, device)\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# Guardar el mejor modelo de este experimento\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, device)\u001b[39m\n\u001b[32m     46\u001b[39m losses[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m].backward()\n\u001b[32m     47\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m total_loss += \u001b[43mlosses\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mloss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m pbar.set_postfix(loss=total_loss/\u001b[38;5;28mlen\u001b[39m(pbar))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# EXPERIMENTO 1: CNN PERSONALIZADA + COMPARACIÓN DE AUMENTO DE DATOS\n",
    "# ================================================================\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "augmentation_pipelines = {\n",
    "    \"light\": light_augmentations,\n",
    "    \"medium\": medium_augmentations,\n",
    "    \"heavy\": heavy_augmentations\n",
    "}\n",
    "\n",
    "best_custom_cnn_model = None\n",
    "best_custom_cnn_acc = -1\n",
    "\n",
    "for name, aug_pipeline in augmentation_pipelines.items():\n",
    "    print(f\"\\n--- Entrenando con aumento de datos: {name.upper()} ---\")\n",
    "\n",
    "    # 1. Crear Datasets y DataLoaders\n",
    "    # Aseguramos que las variables se creen de nuevo en cada iteración\n",
    "    train_dataset_custom = MaskDataset(train_df, root_dir=IMG_DIR, transform=aug_pipeline)\n",
    "    val_dataset_custom = MaskDataset(val_df, root_dir=IMG_DIR, transform=val_transforms)\n",
    "\n",
    "    train_loader_custom = DataLoader(train_dataset_custom, batch_size=32, shuffle=True)\n",
    "    val_loader_custom = DataLoader(val_dataset_custom, batch_size=32, shuffle=False)\n",
    "\n",
    "    # 2. Instanciar el modelo (siempre uno nuevo)\n",
    "    backbone_custom = CustomCNN(output_features_dim=128).to(device)\n",
    "    model_custom = Model(backbone=backbone_custom, backbone_out_features=128).to(device)\n",
    "    optimizer_custom = torch.optim.Adam(model_custom.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # 3. Bucle de entrenamiento\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Época {epoch + 1}/{EPOCHS}\")\n",
    "        train_one_epoch(model_custom, train_loader_custom, optimizer_custom, device)\n",
    "        current_acc = evaluate(model_custom, val_loader_custom, device)\n",
    "\n",
    "        # Guardar el mejor modelo de este experimento\n",
    "        if current_acc > best_custom_cnn_acc:\n",
    "            best_custom_cnn_acc = current_acc\n",
    "            best_custom_cnn_model = copy.deepcopy(model_custom.state_dict())\n",
    "            print(f\"Nuevo mejor modelo de CNN personalizada con Accuracy: {best_custom_cnn_acc:.4f}\")\n",
    "\n",
    "# Guardar el mejor modelo de la CNN personalizada en disco\n",
    "torch.save(best_custom_cnn_model, 'custom_cnn_best.pth')\n",
    "print(\"\\nMejor modelo de CNN personalizada guardado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 2: Transfer Learning\n",
    "\n",
    "Ahora, utilizaremos modelos pre-entrenados como backbones, conectando nuestras cabezas de clasificación y regresión.\n",
    "\n",
    "### 2.1 EfficientNet-B0: Ajuste del Learning Rate\n",
    "Probaremos tres tasas de aprendizaje (`1e-3`, `1e-4`, `1e-5`) para encontrar la que mejor se adapte a la tarea de fine-tuning con EfficientNet. Un learning rate más bajo suele ser preferible para no destruir las características aprendidas por el modelo pre-entrenado.\n",
    "\n",
    "### 2.2 Swin Transformer V2: Ajuste del Optimizador y Weight Decay\n",
    "Para el Swin Transformer, compararemos dos optimizadores:\n",
    "- **Adam:** Un optimizador estándar y robusto.\n",
    "- **AdamW:** Una variante de Adam que desacopla la regularización de `weight_decay` de la actualización del gradiente, lo que a menudo conduce a una mejor generalización.\n",
    "Además, probaremos dos valores de `weight_decay` (`1e-4` y `1e-2`) para analizar el efecto de la regularización L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Entrenando EfficientNet-B0 con LR: 0.001 ---\n",
      "Época 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 11/11 [00:13<00:00,  1.25s/it, loss=0.23] \n",
      "Evaluando: 100%|██████████| 3/3 [00:00<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de validación -> Accuracy: 0.7986, IoU: 0.0047, Average: 0.4016\n",
      "Nuevo mejor modelo EfficientNet con Accuracy: 0.4016\n",
      "Época 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 11/11 [00:13<00:00,  1.24s/it, loss=0.0826]\n",
      "Evaluando: 100%|██████████| 3/3 [00:00<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de validación -> Accuracy: 0.9306, IoU: 0.1443, Average: 0.5374\n",
      "Nuevo mejor modelo EfficientNet con Accuracy: 0.5374\n",
      "Época 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████| 11/11 [00:13<00:00,  1.24s/it, loss=0.0581]\n",
      "Evaluando: 100%|██████████| 3/3 [00:00<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de validación -> Accuracy: 0.9306, IoU: 0.2027, Average: 0.5666\n",
      "Nuevo mejor modelo EfficientNet con Accuracy: 0.5666\n",
      "Época 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  45%|████▌     | 5/11 [00:06<00:07,  1.30s/it, loss=0.0104] "
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# EXPERIMENTO 2.1: EFFICIENTNET-B0 + AJUSTE DE LEARNING RATE\n",
    "# ================================================================\n",
    "EPOCHS = 100\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "\n",
    "best_effnet_model = None\n",
    "best_effnet_acc = -1\n",
    "\n",
    "train_dataset_tl = MaskDataset(train_df, root_dir=IMG_DIR, transform=medium_augmentations)\n",
    "val_dataset_tl = MaskDataset(val_df, root_dir=IMG_DIR, transform=val_transforms)\n",
    "train_loader_tl = DataLoader(train_dataset_tl, batch_size=16, shuffle=True)\n",
    "val_loader_tl = DataLoader(val_dataset_tl, batch_size=16, shuffle=False)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n--- Entrenando EfficientNet-B0 con LR: {lr} ---\")\n",
    "\n",
    "    backbone_effnet = EffNetFeatureExtractor().to(device)\n",
    "    model_effnet = Model(backbone=backbone_effnet, backbone_out_features=1280).to(device)\n",
    "    optimizer_effnet = torch.optim.Adam(model_effnet.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Época {epoch + 1}/{EPOCHS}\")\n",
    "        train_one_epoch(model_effnet, train_loader_tl, optimizer_effnet, device)\n",
    "        current_acc = evaluate(model_effnet, val_loader_tl, device)\n",
    "\n",
    "        if current_acc > best_effnet_acc:\n",
    "            best_effnet_acc = current_acc\n",
    "            best_effnet_model = copy.deepcopy(model_effnet.state_dict())\n",
    "            print(f\"Nuevo mejor modelo EfficientNet con Accuracy: {best_effnet_acc:.4f}\")\n",
    "\n",
    "torch.save(best_effnet_model, 'efficientnet_best.pth')\n",
    "print(\"\\nMejor modelo EfficientNet guardado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Entrenando Swin Transformer con Optimizador: Adam (wd=0.0001) ---\n",
      "Época 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:   0%|          | 0/11 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mÉpoca \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_swin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_tl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_swin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     current_iou = evaluate(model_swin, val_loader_tl, device)\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m current_iou > best_swin_iou:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, device)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbbox\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mclass_id\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     42\u001b[39m     batch[key] = batch[key].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m losses = loss_fn(batch, preds)\n\u001b[32m     46\u001b[39m losses[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m].backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\barsing-pc\\Proyectos\\proyecto-final-aa-iv\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\barsing-pc\\Proyectos\\proyecto-final-aa-iv\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> ty.Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     pred_bbox = \u001b[38;5;28mself\u001b[39m.reg_head(features)\n\u001b[32m     62\u001b[39m     cls_logits = \u001b[38;5;28mself\u001b[39m.cls_head(features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\barsing-pc\\Proyectos\\proyecto-final-aa-iv\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\barsing-pc\\Proyectos\\proyecto-final-aa-iv\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mSwinV2FeatureExtractor.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     x = x.permute(\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     93\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.pool(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\barsing-pc\\Proyectos\\proyecto-final-aa-iv\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\barsing-pc\\Proyectos\\proyecto-final-aa-iv\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\barsing-pc\\Proyectos\\proyecto-final-aa-iv\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\barsing-pc\\Proyectos\\proyecto-final-aa-iv\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\barsing-pc\\Proyectos\\proyecto-final-aa-iv\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\barsing-pc\\Proyectos\\proyecto-final-aa-iv\\venv\\Lib\\site-packages\\torchvision\\models\\swin_transformer.py:111\u001b[39m, in \u001b[36mPatchMergingV2.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m    x (Tensor): input tensor with expected layout of [..., H, W, C]\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[33;03mReturns:\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m    Tensor with layout of [..., H/2, W/2, 2*C]\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    110\u001b[39m x = _patch_merging_pad(x)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ... H/2 W/2 2*C\u001b[39;00m\n\u001b[32m    112\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm(x)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\barsing-pc\\Proyectos\\proyecto-final-aa-iv\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\barsing-pc\\Proyectos\\proyecto-final-aa-iv\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\barsing-pc\\Proyectos\\proyecto-final-aa-iv\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# EXPERIMENTO 2.2: SWIN TRANSFORMER + AJUSTE DE OPTIMIZADOR\n",
    "# ================================================================\n",
    "EPOCHS = 10\n",
    "\n",
    "optimizers_config = [\n",
    "    {\"name\": \"Adam\", \"optim\": torch.optim.Adam, \"wd\": 1e-4},\n",
    "    {\"name\": \"AdamW\", \"optim\": torch.optim.AdamW, \"wd\": 1e-4},\n",
    "    {\"name\": \"AdamW_high_wd\", \"optim\": torch.optim.AdamW, \"wd\": 1e-2},\n",
    "]\n",
    "\n",
    "best_swin_model = None\n",
    "best_swin_acc = -1\n",
    "LEARNING_RATE_SWIN = 1e-5\n",
    "\n",
    "for config in optimizers_config:\n",
    "    print(f\"\\n--- Entrenando Swin Transformer con Optimizador: {config['name']} (wd={config['wd']}) ---\")\n",
    "\n",
    "    backbone_swin = SwinV2FeatureExtractor().to(device)\n",
    "    model_swin = Model(backbone=backbone_swin, backbone_out_features=1024).to(device) # Swin-V2-B tiene 1024 features\n",
    "    optimizer_swin = config[\"optim\"](model_swin.parameters(), lr=LEARNING_RATE_SWIN, weight_decay=config[\"wd\"])\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Época {epoch + 1}/{EPOCHS}\")\n",
    "        train_one_epoch(model_swin, train_loader_tl, optimizer_swin, device)\n",
    "        current_acc = evaluate(model_swin, val_loader_tl, device)\n",
    "\n",
    "        if current_acc > best_swin_acc:\n",
    "            best_swin_acc = current_acc\n",
    "            best_swin_model = copy.deepcopy(model_swin.state_dict())\n",
    "            print(f\"Nuevo mejor modelo Swin Transformer con Accuracy: {best_swin_acc:.4f}\")\n",
    "\n",
    "torch.save(best_swin_model, 'swin_transformer_best.pth')\n",
    "print(\"\\nMejor modelo Swin Transformer guardado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación del Archivo de Submission\n",
    "\n",
    "Finalmente, cargamos el mejor modelo obtenido de todos los experimentos (el que haya alcanzado el mayor IoU en validación) y lo utilizamos para generar las predicciones sobre el conjunto de test. Las predicciones se guardan en un archivo `submission.csv` con el formato requerido por la competencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el mejor modelo para la inferencia...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones: 100%|██████████| 55/55 [00:01<00:00, 50.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Archivo 'submission.csv' generado con éxito.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IMG_4861_mp4-50_jpg.rf.7173e37ed9f62f8939af82323289faf2.jpg</th>\n",
       "      <td>no-mask</td>\n",
       "      <td>186.878479</td>\n",
       "      <td>94.037437</td>\n",
       "      <td>524.567932</td>\n",
       "      <td>325.683716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_CDC-YOUTUBE_mp4-58_jpg.rf.370d5f316397477da0ff4f44799b1da9.jpg</th>\n",
       "      <td>mask</td>\n",
       "      <td>233.317566</td>\n",
       "      <td>189.071991</td>\n",
       "      <td>234.204590</td>\n",
       "      <td>233.019089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_CDC-YOUTUBE_mp4-57_jpg.rf.de4856b9a314980e4113335576f453a8.jpg</th>\n",
       "      <td>mask</td>\n",
       "      <td>231.072601</td>\n",
       "      <td>153.838226</td>\n",
       "      <td>240.381088</td>\n",
       "      <td>206.621063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG_3102_mp4-0_jpg.rf.6a18575fb4bf7f69cc9006b9a5f34e08.jpg</th>\n",
       "      <td>no-mask</td>\n",
       "      <td>134.439713</td>\n",
       "      <td>115.148354</td>\n",
       "      <td>474.103821</td>\n",
       "      <td>385.948669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMG_3094_mp4-34_jpg.rf.11eecb9601680286dc8338d5e8b9acb2.jpg</th>\n",
       "      <td>no-mask</td>\n",
       "      <td>182.659912</td>\n",
       "      <td>105.384048</td>\n",
       "      <td>455.287659</td>\n",
       "      <td>360.595062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      class        xmin  \\\n",
       "filename                                                                  \n",
       "IMG_4861_mp4-50_jpg.rf.7173e37ed9f62f8939af8232...  no-mask  186.878479   \n",
       "video_CDC-YOUTUBE_mp4-58_jpg.rf.370d5f316397477...     mask  233.317566   \n",
       "video_CDC-YOUTUBE_mp4-57_jpg.rf.de4856b9a314980...     mask  231.072601   \n",
       "IMG_3102_mp4-0_jpg.rf.6a18575fb4bf7f69cc9006b9a...  no-mask  134.439713   \n",
       "IMG_3094_mp4-34_jpg.rf.11eecb9601680286dc8338d5...  no-mask  182.659912   \n",
       "\n",
       "                                                          ymin        xmax  \\\n",
       "filename                                                                     \n",
       "IMG_4861_mp4-50_jpg.rf.7173e37ed9f62f8939af8232...   94.037437  524.567932   \n",
       "video_CDC-YOUTUBE_mp4-58_jpg.rf.370d5f316397477...  189.071991  234.204590   \n",
       "video_CDC-YOUTUBE_mp4-57_jpg.rf.de4856b9a314980...  153.838226  240.381088   \n",
       "IMG_3102_mp4-0_jpg.rf.6a18575fb4bf7f69cc9006b9a...  115.148354  474.103821   \n",
       "IMG_3094_mp4-34_jpg.rf.11eecb9601680286dc8338d5...  105.384048  455.287659   \n",
       "\n",
       "                                                          ymax  \n",
       "filename                                                        \n",
       "IMG_4861_mp4-50_jpg.rf.7173e37ed9f62f8939af8232...  325.683716  \n",
       "video_CDC-YOUTUBE_mp4-58_jpg.rf.370d5f316397477...  233.019089  \n",
       "video_CDC-YOUTUBE_mp4-57_jpg.rf.de4856b9a314980...  206.621063  \n",
       "IMG_3102_mp4-0_jpg.rf.6a18575fb4bf7f69cc9006b9a...  385.948669  \n",
       "IMG_3094_mp4-34_jpg.rf.11eecb9601680286dc8338d5...  360.595062  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================================\n",
    "# GENERACIÓN DE SUBMISSION\n",
    "# ===============================================\n",
    "\n",
    "test_df = pd.read_csv(osp.join(DATA_DIR, \"test.csv\"))\n",
    "\n",
    "test_transforms = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE)\n",
    "])\n",
    "\n",
    "test_dataset = MaskDataset(test_df, root_dir=IMG_DIR, transform=test_transforms, labeled=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(\"Cargando el mejor modelo para la inferencia...\")\n",
    "best_backbone = EffNetFeatureExtractor().to(device) # Cambiar si otro modelo fue el mejor\n",
    "best_model = Model(backbone=best_backbone, backbone_out_features=1280).to(device)\n",
    "best_model.load_state_dict(torch.load('efficientnet_best.pth'))\n",
    "best_model.eval()\n",
    "\n",
    "filenames, class_preds, xmins, ymins, xmaxs, ymaxs = [], [], [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(test_loader, desc=\"Generando predicciones\")):\n",
    "        image = batch['image'].to(device)\n",
    "        preds = best_model(image)\n",
    "\n",
    "        class_id = torch.argmax(preds['class_id'], dim=1).cpu().item()\n",
    "        bbox = preds['bbox'].cpu().numpy()[0]\n",
    "\n",
    "        xmin = bbox[0] * w_real\n",
    "        ymin = bbox[1] * h_real\n",
    "        xmax = bbox[2] * w_real\n",
    "        ymax = bbox[3] * h_real\n",
    "\n",
    "        filenames.append(test_df.filename.iloc[i])\n",
    "        class_preds.append(id2obj[class_id])\n",
    "        xmins.append(xmin)\n",
    "        ymins.append(ymin)\n",
    "        xmaxs.append(xmax)\n",
    "        ymaxs.append(ymax)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'class': class_preds,\n",
    "    'xmin': xmins,\n",
    "    'ymin': ymins,\n",
    "    'xmax': xmaxs,\n",
    "    'ymax': ymaxs\n",
    "})\n",
    "\n",
    "submission_df = submission_df.set_index('filename')\n",
    "\n",
    "submission_df.to_csv('submission.csv')\n",
    "\n",
    "print(\"\\nArchivo 'submission.csv' generado con éxito.\")\n",
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
